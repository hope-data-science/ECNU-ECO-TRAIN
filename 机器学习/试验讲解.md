------

# **整体背景**

这个案例是一个完整的 **Iris 数据集多分类任务**，使用 **tidymodels 框架**实现，包括以下特点：

- 使用三个模型：多类逻辑回归（`multinom`/nnet）、随机森林（`ranger`）、KNN（`kknn`）。
- 对随机森林和 KNN 进行 **超参数调优**，使用 5 折交叉验证。
- 模型评估包括 **准确率、混淆矩阵、ROC AUC**。
- 对随机森林模型进行 **变量重要性分析**（VIP）。

------

# **1. 数据加载与划分**

```r
data(iris)
iris <- as.data.frame(iris)
iris_split <- initial_split(iris, prop = 0.8, strata = Species)
iris_train <- training(iris_split)
iris_test  <- testing(iris_split)
```

- 使用内置的 `iris` 数据集。
- `initial_split()` 将数据集拆分为训练集（80%）和测试集（20%）。
- `strata = Species` 表示 **按类别分层抽样**，确保每个类别在训练集和测试集中比例相同。
- `training()` 和 `testing()` 提取训练集和测试集。

------

# **2. 数据预处理 Recipe**

```r
iris_rec <- recipe(Species ~ ., data = iris_train) %>%
  step_normalize(all_numeric_predictors())
```

- 使用 **`recipes`** 进行数据预处理。
- `Species ~ .` 指明目标变量为 `Species`，所有其他列为预测变量。
- `step_normalize()` 对数值特征进行标准化，使每个特征均值为 0、标准差为 1。
  - 对 KNN 特别重要，因为距离计算受特征尺度影响。

------

# **3. 模型规格 (parsnip)**

```r
# 多类逻辑回归
log_mod <- multinom_reg(mode = "classification") %>%
  set_engine("nnet")

# 随机森林
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
  set_engine("ranger", importance = "permutation") %>%
  set_mode("classification")

# KNN
knn_mod <- nearest_neighbor(neighbors = tune(), weight_func = "rectangular", dist_power = 2) %>%
  set_engine("kknn") %>%
  set_mode("classification")
```

- **parsnip** 提供统一接口定义模型。
- **逻辑回归**：
  - `multinom_reg()` 支持多类分类。
  - `set_engine("nnet")` 使用 `nnet::multinom()`。
  - 没有需要调参的参数。
- **随机森林**：
  - `mtry`（每棵树随机选择特征数）和 `min_n`（节点最小样本数）需要调参。
  - `trees = 500` 设置树的数量。
  - `importance = "permutation"` 用于计算基于置换的变量重要性。
- **KNN**：
  - `neighbors` 需要调参。
  - `weight_func = "rectangular"` 表示均等权重。均等的权重，就说所有的邻近点的*权重*都是相等的。不均等的权重，距离近的点比距离远的点的影响大。
  - `dist_power = 2` 表示欧氏距离。

------

# **4. Workflow 绑定 recipe 与模型**

```r
log_wf <- workflow() %>% add_model(log_mod) %>% add_recipe(iris_rec)
rf_wf  <- workflow() %>% add_model(rf_mod)  %>% add_recipe(iris_rec)
knn_wf <- workflow() %>% add_model(knn_mod) %>% add_recipe(iris_rec)
```

- `workflow()` 将 **数据预处理步骤（recipe）** 与 **模型规格（parsnip）** 绑定在一起。
- 这样在后续调参和拟合时，可以统一处理。

------

# **5. 交叉验证折**

```r
iris_folds <- vfold_cv(iris_train, v = 5, strata = Species)
metrics <- metric_set(accuracy, roc_auc)
ctrl <- control_grid(save_pred = TRUE)
```

- `vfold_cv()` 创建 **5 折交叉验证**，按 `Species` 分层。
- `metric_set()` 指定评价指标：**准确率** 和 **ROC AUC**。
- `control_grid(save_pred = TRUE)` 保存每次交叉验证预测结果，用于后续分析。

------

# **6. 随机森林调参**

```r
rf_params <- rf_wf %>% extract_parameter_set_dials()
predictors_df <- iris_train %>% select(-Species)
rf_params_final <- dials::finalize(rf_params, x = predictors_df)
rf_grid <- dials::grid_random(rf_params_final, size = 20)
rf_res <- tune_grid(rf_wf, resamples = iris_folds, grid = rf_grid,
                    metrics = metrics, control = ctrl)
best_rf <- select_best(rf_res, metric = "accuracy")
```

- **调参流程**：
  1. `extract_parameter_set_dials()` 提取 workflow 中可调参数。
  2. `finalize()` 将未知参数（如 `mtry`）根据训练数据确定上限。
  3. `grid_random()` 生成随机搜索网格。
  4. `tune_grid()` 对网格进行交叉验证，计算每组参数的评价指标。
  5. `select_best()` 根据准确率选出最佳参数。

------

# **7. KNN 调参**

流程类似随机森林：

```r
knn_params <- knn_wf %>% extract_parameter_set_dials()
knn_params_final <- dials::finalize(knn_params, x = predictors_df)
knn_grid <- dials::grid_regular(knn_params_final, levels = 10)
knn_res <- tune_grid(knn_wf, resamples = iris_folds, grid = knn_grid,
                     metrics = metrics, control = ctrl)
best_knn <- select_best(knn_res, metric = "accuracy")
```

- `grid_regular()` 按均匀间隔生成调参网格。
- 其他逻辑与随机森林相同。

------

# **8. 训练最终模型**

```r
# 逻辑回归无需调参
log_fit <- fit(log_wf, data = iris_train)

# 随机森林和 KNN 使用最佳参数 finalize
final_rf_wf <- finalize_workflow(rf_wf, best_rf)
rf_fit <- fit(final_rf_wf, data = iris_train)

final_knn_wf <- finalize_workflow(knn_wf, best_knn)
knn_fit <- fit(final_knn_wf, data = iris_train)
```

- `fit()` 直接训练模型。
- `finalize_workflow()` 将最佳参数固定在 workflow 中。
- 对逻辑回归，因为没有调参，直接用 `fit()`。

------

# **9. 测试集预测与评估**

```r
eval_model <- function(fit, test_data, model_name = "model") {
  pred_class <- predict(fit, test_data)
  pred_prob  <- predict(fit, test_data, type = "prob")
  res <- bind_cols(test_data, pred_class, pred_prob)
  print(conf_mat(res, truth = Species, estimate = .pred_class))
  print(accuracy(res, truth = Species, estimate = .pred_class))
  auc_res <- tryCatch({
    roc_auc(res, truth = Species, .pred_setosa, .pred_versicolor, .pred_virginica,
            estimator = "macro_weighted")
  }, error = function(e) NA)
  print(auc_res)
  return(res)
}

res_log <- eval_model(log_fit, iris_test, "Multinomial (nnet)")
res_rf  <- eval_model(rf_fit, iris_test, "Random Forest")
res_knn <- eval_model(knn_fit, iris_test, "KNN")
```

- `predict()` 分别生成类别预测和概率预测。
- `conf_mat()` 输出混淆矩阵。
- `accuracy()` 输出准确率。
- `roc_auc()` 对多分类使用 `macro_weighted` 计算平均 ROC AUC。
- `tryCatch()` 防止报错（某些模型可能不支持 ROC 计算）。

------

# **10. 绘制 ROC 曲线**

```r
roc_res_rf <- roc_curve(res_rf, truth = Species,
                        .pred_setosa, .pred_versicolor, .pred_virginica,
                        estimator = "hand_till")
autoplot(roc_res_rf) + ggtitle("Random Forest ROC (multi-class)")
```

- 多分类 ROC 使用 **Hand-Till 方法**。
- 将每个类别的预测概率列传入。
- `autoplot()` 绘图。

------

# **11. 随机森林变量重要性**

```r
rf_parsnip <- extract_fit_parsnip(rf_fit)
ranger_obj <- rf_parsnip$fit
vip(ranger_obj) + ggtitle("Random Forest - Variable Importance (vip)")
```

- `extract_fit_parsnip()` 提取底层 `ranger` 模型对象。
- `vip()` 绘制变量重要性图（基于训练时设置的 `importance = "permutation"`）。
- 有助于解释模型，观察哪个特征对分类最重要。

------

# **总结**

这个脚本完整展示了 tidymodels 的 **机器学习全流程**：

1. 数据加载与划分（训练/测试 + 分层）。
2. 数据预处理（Recipe + 标准化）。
3. 模型定义（parsnip）：
   - 无调参逻辑回归。
   - 可调参随机森林、KNN。
4. Workflow 绑定 recipe 与模型。
5. 交叉验证与调参（tune_grid）。
6. 选择最佳参数并训练最终模型。
7. 测试集预测与评估（混淆矩阵、准确率、ROC）。
8. 模型可解释性（随机森林 VIP）。

整个流程体现了 **现代 R 语言机器学习最佳实践**，同时可以拓展到更多模型和更复杂的数据集。

------

